{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN Classification\n",
    "\n",
    "### **2018/12/6 CoE 202 Activity 6**<br/>\n",
    "<br/>\n",
    "\n",
    "***Tip> shotcuts for Jupyter Notebook***\n",
    "* Shift + Enter : run cell and select below\n",
    "\n",
    "***Library***\n",
    "* Numpy: Fundamenta package for scientific computing with Python\n",
    "* Tensorflow: An open source machine learning library for research and production\n",
    "* String : contains a number of functions to process standard Python strings(a series of characters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "model_save_path = 'tmp/model.ckpt'\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_input = len(all_letters)\n",
    "n_hidden = 128 # hidden layer features\n",
    "max_sequence_length = 19 # maximum number of characters is 19\n",
    "\n",
    "\n",
    "alphabet = all_letters\n",
    "ethnicities = ['Chinese', 'Japanese', 'Vietnamese', 'Korean', 'Arabic','Czech','Dutch','English','French','German','Greek','Irish','Italian','Polish','Portuguese','Russian','Scottish','Spanish']\n",
    "n_classes = len(ethnicities) # the number of classes\n",
    "\n",
    "name_strings = []\n",
    "ethnicity_strings = []\n",
    "str_list = []\n",
    "names_list = []\n",
    "ethnicity_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_one_hot(name, max_sequence_length):\n",
    "    result = []\n",
    "    for char in name:\n",
    "        v = np.zeros(n_input, dtype=np.int) # count space as a character\n",
    "        v[alphabet.index(char)] = 1\n",
    "        result.append(v)\n",
    "    while len(result) < max_sequence_length:\n",
    "        result.append(np.zeros(n_input, dtype=np.int))\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ethnicity_one_hot(ethnicity):\n",
    "    v = np.zeros(n_classes, dtype=np.int)\n",
    "    v[ethnicities.index(ethnicity)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('names_revised.csv', 'r') as csv:\n",
    "    for line in csv:       \n",
    "        l = [s.strip() for s in line.split(',')] # lowercase L, not capital i , l['name', 'ehnicity']\n",
    "        if(l[1] in ethnicities):\n",
    "            name_strings.append(l[0])\n",
    "            ethnicity_strings.append(l[1])\n",
    "            if len(l[0]) > max_sequence_length:\n",
    "                l[0] = l[0][:max_sequence_length]\n",
    "            names_list.append(name_one_hot(l[0], max_sequence_length)) # one-hot vector of each characters of name\n",
    "            ethnicity_list.append(ethnicity_one_hot(l[1])) # one-hot vector of ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state() # use the same random number generator state\n",
    "np.random.shuffle(names_list)     # when shuffling the two lists\n",
    "np.random.set_state(rng_state)    # they are effectively shuffled in parallel so that inputs still correspond to outputs after shuffling\n",
    "np.random.shuffle(ethnicity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = len(names_list) \n",
    "train_size = np.int(size*2/3) \n",
    "\n",
    "training_X = np.array(names_list[:train_size])\n",
    "training_y = np.array(ethnicity_list[:train_size])\n",
    "testing_X = np.array(names_list[train_size:])\n",
    "testing_y = np.array(ethnicity_list[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, max_sequence_length, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_weights = weight_variable([n_hidden, n_classes])\n",
    "out_biases = bias_variable([n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-2428095775f8>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "# Basic RNN\n",
    "#cells = tf.contrib.rnn.BasicRNNCell(num_units = 128)\n",
    "# LSTM\n",
    "cells = tf.contrib.rnn.BasicLSTMCell(num_units = 128)\n",
    "# GRU\n",
    "#cells = tf.contrib.rnn.GRUCell(num_units = 128)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cells, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.matmul(outputs[:,-1,:], out_weights) + out_biases # predict y based on final rnn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_, labels=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax\n",
    "pred = tf.nn.softmax(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 10, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 20, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 30, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 40, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 50, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 60, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 70, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 80, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 90, training accuracy 0.464499\n",
      "testing accuracy 0.47675982\n",
      "step 100, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 110, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 120, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 130, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 140, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 150, training accuracy 0.466517\n",
      "testing accuracy 0.4797489\n",
      "step 160, training accuracy 0.466517\n",
      "testing accuracy 0.47959948\n",
      "step 170, training accuracy 0.489611\n",
      "testing accuracy 0.5020176\n",
      "step 180, training accuracy 0.491928\n",
      "testing accuracy 0.5084442\n",
      "step 190, training accuracy 0.564126\n",
      "testing accuracy 0.56987\n",
      "step 200, training accuracy 0.603064\n",
      "testing accuracy 0.5987147\n",
      "step 210, training accuracy 0.63438\n",
      "testing accuracy 0.6318936\n",
      "step 220, training accuracy 0.642302\n",
      "testing accuracy 0.64355105\n",
      "step 230, training accuracy 0.660015\n",
      "testing accuracy 0.65804815\n",
      "step 240, training accuracy 0.668685\n",
      "testing accuracy 0.66312957\n",
      "step 250, training accuracy 0.67997\n",
      "testing accuracy 0.67732775\n",
      "step 260, training accuracy 0.640807\n",
      "testing accuracy 0.63338816\n",
      "step 270, training accuracy 0.680942\n",
      "testing accuracy 0.67508596\n",
      "step 280, training accuracy 0.701794\n",
      "testing accuracy 0.6975041\n",
      "step 290, training accuracy 0.719058\n",
      "testing accuracy 0.71155286\n",
      "step 300, training accuracy 0.719656\n",
      "testing accuracy 0.7037812\n",
      "step 310, training accuracy 0.739686\n",
      "testing accuracy 0.729039\n",
      "step 320, training accuracy 0.753438\n",
      "testing accuracy 0.7368106\n",
      "step 330, training accuracy 0.765695\n",
      "testing accuracy 0.7483186\n",
      "step 340, training accuracy 0.774813\n",
      "testing accuracy 0.7547452\n",
      "step 350, training accuracy 0.706951\n",
      "testing accuracy 0.6919743\n",
      "step 360, training accuracy 0.713677\n",
      "testing accuracy 0.702735\n",
      "step 370, training accuracy 0.755979\n",
      "testing accuracy 0.738903\n",
      "step 380, training accuracy 0.776981\n",
      "testing accuracy 0.75683755\n",
      "step 390, training accuracy 0.791405\n",
      "testing accuracy 0.7690928\n",
      "step 400, training accuracy 0.804036\n",
      "testing accuracy 0.7770139\n",
      "step 410, training accuracy 0.804559\n",
      "testing accuracy 0.77686447\n",
      "step 420, training accuracy 0.817937\n",
      "testing accuracy 0.7835899\n",
      "step 430, training accuracy 0.828102\n",
      "testing accuracy 0.7886714\n",
      "step 440, training accuracy 0.835575\n",
      "testing accuracy 0.79225826\n",
      "step 450, training accuracy 0.819806\n",
      "testing accuracy 0.7782095\n",
      "step 460, training accuracy 0.844021\n",
      "testing accuracy 0.79360336\n",
      "step 470, training accuracy 0.854783\n",
      "testing accuracy 0.7965924\n",
      "step 480, training accuracy 0.856577\n",
      "testing accuracy 0.79195935\n",
      "step 490, training accuracy 0.86846\n",
      "testing accuracy 0.7942012\n",
      "step 500, training accuracy 0.868909\n",
      "testing accuracy 0.79091316\n",
      "step 510, training accuracy 0.866667\n",
      "testing accuracy 0.8006277\n",
      "step 520, training accuracy 0.844768\n",
      "testing accuracy 0.78792405\n",
      "step 530, training accuracy 0.873244\n",
      "testing accuracy 0.7965924\n",
      "step 540, training accuracy 0.88991\n",
      "testing accuracy 0.8006277\n",
      "step 550, training accuracy 0.901644\n",
      "testing accuracy 0.8004783\n",
      "step 560, training accuracy 0.884604\n",
      "testing accuracy 0.79748917\n",
      "step 570, training accuracy 0.894021\n",
      "testing accuracy 0.80346733\n",
      "step 580, training accuracy 0.908221\n",
      "testing accuracy 0.8009266\n",
      "step 590, training accuracy 0.922795\n",
      "testing accuracy 0.7979375\n",
      "step 600, training accuracy 0.929073\n",
      "testing accuracy 0.7982364\n",
      "step 610, training accuracy 0.905531\n",
      "testing accuracy 0.78822297\n",
      "step 620, training accuracy 0.920179\n",
      "testing accuracy 0.79061425\n",
      "step 630, training accuracy 0.935575\n",
      "testing accuracy 0.79629356\n",
      "step 640, training accuracy 0.945067\n",
      "testing accuracy 0.7942012\n",
      "step 650, training accuracy 0.952616\n",
      "testing accuracy 0.7939023\n",
      "step 660, training accuracy 0.957399\n",
      "testing accuracy 0.79360336\n",
      "step 670, training accuracy 0.890359\n",
      "testing accuracy 0.7655059\n",
      "step 680, training accuracy 0.932511\n",
      "testing accuracy 0.78897023\n",
      "step 690, training accuracy 0.950972\n",
      "testing accuracy 0.7885219\n",
      "step 700, training accuracy 0.95994\n",
      "testing accuracy 0.7912121\n",
      "step 710, training accuracy 0.966517\n",
      "testing accuracy 0.7901659\n",
      "step 720, training accuracy 0.969208\n",
      "testing accuracy 0.78822297\n",
      "step 730, training accuracy 0.9713\n",
      "testing accuracy 0.7868779\n",
      "step 740, training accuracy 0.972272\n",
      "testing accuracy 0.7873263\n",
      "step 750, training accuracy 0.972272\n",
      "testing accuracy 0.7871768\n",
      "step 760, training accuracy 0.973617\n",
      "testing accuracy 0.78568226\n",
      "step 770, training accuracy 0.974439\n",
      "testing accuracy 0.784935\n",
      "step 780, training accuracy 0.975187\n",
      "testing accuracy 0.78284264\n",
      "step 790, training accuracy 0.97571\n",
      "testing accuracy 0.783291\n",
      "step 800, training accuracy 0.975934\n",
      "testing accuracy 0.78418773\n",
      "step 810, training accuracy 0.976233\n",
      "testing accuracy 0.7822448\n",
      "step 820, training accuracy 0.976682\n",
      "testing accuracy 0.7819459\n",
      "step 830, training accuracy 0.976831\n",
      "testing accuracy 0.78164697\n",
      "step 840, training accuracy 0.97713\n",
      "testing accuracy 0.7807503\n",
      "step 850, training accuracy 0.97728\n",
      "testing accuracy 0.78045136\n",
      "step 860, training accuracy 0.977504\n",
      "testing accuracy 0.78164697\n",
      "step 870, training accuracy 0.977803\n",
      "testing accuracy 0.7813481\n",
      "step 880, training accuracy 0.977877\n",
      "testing accuracy 0.7794052\n",
      "step 890, training accuracy 0.977877\n",
      "testing accuracy 0.78179646\n",
      "step 900, training accuracy 0.977877\n",
      "testing accuracy 0.78015244\n",
      "step 910, training accuracy 0.977877\n",
      "testing accuracy 0.7813481\n",
      "step 920, training accuracy 0.977952\n",
      "testing accuracy 0.78164697\n",
      "step 930, training accuracy 0.978027\n",
      "testing accuracy 0.7813481\n",
      "step 940, training accuracy 0.978176\n",
      "testing accuracy 0.7820954\n",
      "step 950, training accuracy 0.978251\n",
      "testing accuracy 0.78179646\n",
      "step 960, training accuracy 0.978401\n",
      "testing accuracy 0.7822448\n",
      "step 970, training accuracy 0.978401\n",
      "testing accuracy 0.7811986\n",
      "step 980, training accuracy 0.978401\n",
      "testing accuracy 0.7825437\n",
      "step 990, training accuracy 0.727877\n",
      "testing accuracy 0.7021372\n",
      "step 1000, training accuracy 0.757848\n",
      "testing accuracy 0.729039\n",
      "Model saved in file: tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 1000\n",
    "\n",
    "for _ in range(n_epoch+1):\n",
    "    sess.run(train_step, feed_dict={X: training_X, y: training_y})\n",
    "    if _%10 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={X:training_X, y:training_y})\n",
    "        print(\"step %d, training accuracy %g\"%(_, train_accuracy))\n",
    "        test_accuracy = accuracy.eval(feed_dict={X:testing_X, y:testing_y})\n",
    "        print(\"testing accuracy\", test_accuracy)\n",
    "saver.save(sess, model_save_path)\n",
    "print(\"Model saved in file: %s\" % model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a last name (max 19 letters):cdascdasdcads\n",
      "\n",
      "(English): 0.8648\n",
      "(Scottish): 0.1340\n",
      "(Irish): 0.0008\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i<5:\n",
    "    input_name = input('Enter a last name (max 19 letters):')\n",
    "   \n",
    "    while len(input_name) > max_sequence_length or len(input_name) == 0:\n",
    "        input_name = raw_input('Invalid input. Enter a last name (max 19 letters):')\n",
    "   \n",
    "    result=pred.eval(feed_dict={X: np.expand_dims(name_one_hot(input_name, 19), axis=0)})[0]\n",
    "    idx = np.argsort(result)[::-1]\n",
    "    print(\"\\n(%s): %.4f\" % (ethnicities[idx[0]], result[idx[0]]))\n",
    "    print(\"(%s): %.4f\" % (ethnicities[idx[1]], result[idx[1]]))\n",
    "    print(\"(%s): %.4f\" % (ethnicities[idx[2]], result[idx[2]]))\n",
    "    print(\"==========================================\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class Report\n",
    "\n",
    "**Use GRU, LSTM and Simple RNN functions for training . Compare each of results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coe202",
   "language": "python",
   "name": "coe202"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
